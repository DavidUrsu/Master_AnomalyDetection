{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6db03f",
   "metadata": {},
   "source": [
    "# lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from pyod.models.pca import PCA as PyOD_PCA\n",
    "from pyod.models.kpca import KPCA\n",
    "from pyod.utils.utility import standardizer\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5847649",
   "metadata": {},
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec05d6a",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab25b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [5, 10, 2]\n",
    "cov = [[3, 2, 2], [2, 10, 1], [2, 1, 2]]\n",
    "X = np.random.multivariate_normal(mean, cov, 500)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c='blue', alpha=0.6)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('3D Dataset')\n",
    "plt.show()\n",
    "\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "cov_matrix = (X_centered.T @ X_centered) / len(X)\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022c78a",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variance = np.sum(eigenvalues)\n",
    "explained_ratio = eigenvalues / total_variance\n",
    "cumulative_ratio = np.cumsum(explained_ratio)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.bar(range(1, 4), explained_ratio, alpha=0.7, label='Individual Variance', color='steelblue')\n",
    "\n",
    "ax.step(range(1, 4), cumulative_ratio, where='mid', label='Cumulative Variance', color='red', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Explained Variance Ratio')\n",
    "ax.set_title('Cumulative and Individual Explained Variance')\n",
    "ax.set_xticks([1, 2, 3])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Individual variance ratios:\", explained_ratio)\n",
    "print(\"Cumulative variance ratios:\", cumulative_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c01f0",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = X_centered @ eigenvectors\n",
    "\n",
    "contamination = 0.1\n",
    "\n",
    "pc3_values = X_transformed[:, 2]\n",
    "pc3_mean = np.mean(pc3_values)\n",
    "pc3_deviation = np.abs(pc3_values - pc3_mean)\n",
    "\n",
    "threshold_pc3 = np.quantile(pc3_deviation, 1 - contamination)\n",
    "y_pred_pc3 = (pc3_deviation > threshold_pc3).astype(int)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[y_pred_pc3==0, 0], X[y_pred_pc3==0, 1], X[y_pred_pc3==0, 2], c='blue', alpha=0.6, label='Normal')\n",
    "ax.scatter(X[y_pred_pc3==1, 0], X[y_pred_pc3==1, 1], X[y_pred_pc3==1, 2], c='red', alpha=0.8, label='Anomaly', s=50)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Outliers based on 3rd Principal Component')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"anomalies detected: {sum(y_pred_pc3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc2_values = X_transformed[:, 1]\n",
    "pc2_mean = np.mean(pc2_values)\n",
    "pc2_deviation = np.abs(pc2_values - pc2_mean)\n",
    "\n",
    "threshold_pc2 = np.quantile(pc2_deviation, 1 - contamination)\n",
    "y_pred_pc2 = (pc2_deviation > threshold_pc2).astype(int)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[y_pred_pc2==0, 0], X[y_pred_pc2==0, 1], X[y_pred_pc2==0, 2], c='blue', alpha=0.6, label='Normal')\n",
    "ax.scatter(X[y_pred_pc2==1, 0], X[y_pred_pc2==1, 1], X[y_pred_pc2==1, 2], c='red', alpha=0.8, label='Anomaly', s=50)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Outliers based on 2nd Principal Component')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"anomalies detected: {sum(y_pred_pc2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4157e",
   "metadata": {},
   "source": [
    "### 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a240fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = X_transformed / np.sqrt(eigenvalues)\n",
    "\n",
    "anomaly_scores = np.sum(X_normalized ** 2, axis=1)\n",
    "\n",
    "threshold_all = np.quantile(anomaly_scores, 1 - contamination)\n",
    "y_pred_all = (anomaly_scores > threshold_all).astype(int)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[y_pred_all==0, 0], X[y_pred_all==0, 1], X[y_pred_all==0, 2], c='blue', alpha=0.6, label='Normal')\n",
    "ax.scatter(X[y_pred_all==1, 0], X[y_pred_all==1, 1], X[y_pred_all==1, 2], c='red', alpha=0.8, label='Anomaly', s=50)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Outliers based on Normalized Distance (All Principal Components)')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"anomalies detected: {sum(y_pred_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508e792",
   "metadata": {},
   "source": [
    "## Ex 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68272cc3",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aafedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_data = loadmat('../lab3/shuttle.mat')\n",
    "X_shuttle = mat_data['X']\n",
    "y_shuttle = mat_data['y'].ravel()\n",
    "\n",
    "print(f\"Number of outliers: {sum(y_shuttle)} ({sum(y_shuttle)/len(y_shuttle)*100:.2f}%)\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_shuttle, y_shuttle, train_size=0.6, random_state=42, stratify=y_shuttle\n",
    ")\n",
    "\n",
    "contamination_rate = sum(y_train) / len(y_train)\n",
    "print(f\"Training contamination rate: {contamination_rate:.4f}\")\n",
    "\n",
    "X_train_std, X_test_std = standardizer(X_train, X_test)\n",
    "\n",
    "pca_model = PyOD_PCA(contamination=contamination_rate)\n",
    "pca_model.fit(X_train_std)\n",
    "\n",
    "explained_var = pca_model.explained_variance_\n",
    "explained_ratio = explained_var / np.sum(explained_var)\n",
    "cumulative_ratio = np.cumsum(explained_ratio)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(range(1, len(explained_ratio)+1), explained_ratio, alpha=0.7, label='Individual Variance', color='steelblue')\n",
    "ax.step(range(1, len(cumulative_ratio)+1), cumulative_ratio, where='mid', label='Cumulative Variance', color='red', linewidth=2)\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Explained Variance Ratio')\n",
    "ax.set_title('PCA - Cumulative and Individual Explained Variance (Shuttle Dataset)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ade792",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f332b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_pca = pca_model.labels_\n",
    "y_pred_test_pca = pca_model.predict(X_test_std)\n",
    "\n",
    "ba_train_pca = balanced_accuracy_score(y_train, y_pred_train_pca)\n",
    "ba_test_pca = balanced_accuracy_score(y_test, y_pred_test_pca)\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(f\"Train Balanced Accuracy: {ba_train_pca:.4f}\")\n",
    "print(f\"Test Balanced Accuracy: {ba_test_pca:.4f}\")\n",
    "\n",
    "kpca_model = KPCA(contamination=contamination_rate)\n",
    "kpca_model.fit(X_train_std)\n",
    "\n",
    "y_pred_train_kpca = kpca_model.labels_\n",
    "y_pred_test_kpca = kpca_model.predict(X_test_std)\n",
    "\n",
    "ba_train_kpca = balanced_accuracy_score(y_train, y_pred_train_kpca)\n",
    "ba_test_kpca = balanced_accuracy_score(y_test, y_pred_test_kpca)\n",
    "\n",
    "print(\"\\nKPCA Results:\")\n",
    "print(f\"Train Balanced Accuracy: {ba_train_kpca:.4f}\")\n",
    "print(f\"Test Balanced Accuracy: {ba_test_kpca:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149222a7",
   "metadata": {},
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f7095",
   "metadata": {},
   "source": [
    "### 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a1c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "mat_data = loadmat('../lab3/shuttle.mat')\n",
    "X_shuttle = mat_data['X']\n",
    "y_shuttle = mat_data['y'].ravel()\n",
    "\n",
    "X_train_ae, X_test_ae, y_train_ae, y_test_ae = train_test_split(\n",
    "    X_shuttle, y_shuttle, test_size=0.5, random_state=42, stratify=y_shuttle\n",
    ")\n",
    "\n",
    "X_min = X_train_ae.min(axis=0)\n",
    "X_max = X_train_ae.max(axis=0)\n",
    "X_train_norm = (X_train_ae - X_min) / (X_max - X_min + 1e-8)\n",
    "X_test_norm = (X_test_ae - X_min) / (X_max - X_min + 1e-8)\n",
    "\n",
    "print(f\"Training set: {X_train_norm.shape}\")\n",
    "print(f\"Test set: {X_test_norm.shape}\")\n",
    "print(f\"Contamination rate: {sum(y_train_ae)/len(y_train_ae):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfda921",
   "metadata": {},
   "source": [
    "### 3.2 Design Autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = keras.Sequential([\n",
    "            keras.layers.Dense(8, activation='relu'),\n",
    "            keras.layers.Dense(5, activation='relu'),\n",
    "            keras.layers.Dense(3, activation='relu')\n",
    "        ])\n",
    "        \n",
    "        self.decoder = keras.Sequential([\n",
    "            keras.layers.Dense(5, activation='relu'),\n",
    "            keras.layers.Dense(8, activation='relu'),\n",
    "            keras.layers.Dense(9, activation='sigmoid')\n",
    "        ])\n",
    "    \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0520a88",
   "metadata": {},
   "source": [
    "### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d99372",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder()\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train_norm, X_train_norm,\n",
    "    epochs=100,\n",
    "    batch_size=1024,\n",
    "    validation_data=(X_test_norm, X_test_norm),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Autoencoder Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262022ab",
   "metadata": {},
   "source": [
    "### 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reconstructed = autoencoder.predict(X_train_norm)\n",
    "test_reconstructed = autoencoder.predict(X_test_norm)\n",
    "\n",
    "train_errors = np.mean((X_train_norm - train_reconstructed) ** 2, axis=1)\n",
    "test_errors = np.mean((X_test_norm - test_reconstructed) ** 2, axis=1)\n",
    "\n",
    "contamination_ae = sum(y_train_ae) / len(y_train_ae)\n",
    "threshold = np.quantile(train_errors, 1 - contamination_ae)\n",
    "\n",
    "y_pred_train_ae = (train_errors > threshold).astype(int)\n",
    "y_pred_test_ae = (test_errors > threshold).astype(int)\n",
    "\n",
    "ba_train_ae = balanced_accuracy_score(y_train_ae, y_pred_train_ae)\n",
    "ba_test_ae = balanced_accuracy_score(y_test_ae, y_pred_test_ae)\n",
    "\n",
    "print(f\"Threshold: {threshold:.6f}\")\n",
    "print(f\"Training Balanced Accuracy: {ba_train_ae:.4f}\")\n",
    "print(f\"Test Balanced Accuracy: {ba_test_ae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35039732",
   "metadata": {},
   "source": [
    "## Ex 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4be26e",
   "metadata": {},
   "source": [
    "### 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb007be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_mnist, _), (x_test_mnist, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train_mnist = x_train_mnist.astype('float32') / 255.0\n",
    "x_test_mnist = x_test_mnist.astype('float32') / 255.0\n",
    "\n",
    "x_train_mnist = x_train_mnist[..., np.newaxis]\n",
    "x_test_mnist = x_test_mnist[..., np.newaxis]\n",
    "\n",
    "noise_factor = 0.35\n",
    "x_train_noisy = x_train_mnist + noise_factor * tf.random.normal(shape=x_train_mnist.shape)\n",
    "x_test_noisy = x_test_mnist + noise_factor * tf.random.normal(shape=x_test_mnist.shape)\n",
    "\n",
    "x_train_noisy = tf.clip_by_value(x_train_noisy, 0.0, 1.0)\n",
    "x_test_noisy = tf.clip_by_value(x_test_noisy, 0.0, 1.0)\n",
    "\n",
    "print(f\"Training data shape: {x_train_mnist.shape}\")\n",
    "print(f\"Test data shape: {x_test_mnist.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659b0c6",
   "metadata": {},
   "source": [
    "### 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = keras.Sequential([\n",
    "            keras.layers.Conv2D(8, (3, 3), activation='relu', strides=2, padding='same'),\n",
    "            keras.layers.Conv2D(4, (3, 3), activation='relu', strides=2, padding='same')\n",
    "        ])\n",
    "        \n",
    "        self.decoder = keras.Sequential([\n",
    "            keras.layers.Conv2DTranspose(4, (3, 3), activation='relu', strides=2, padding='same'),\n",
    "            keras.layers.Conv2DTranspose(8, (3, 3), activation='relu', strides=2, padding='same'),\n",
    "            keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')\n",
    "        ])\n",
    "    \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ede59",
   "metadata": {},
   "source": [
    "### 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_autoencoder = ConvAutoencoder()\n",
    "conv_autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history_conv = conv_autoencoder.fit(\n",
    "    x_train_mnist, x_train_mnist,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_test_mnist, x_test_mnist),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_reconstructed_conv = conv_autoencoder.predict(x_train_mnist)\n",
    "train_errors_conv = np.mean((x_train_mnist - train_reconstructed_conv) ** 2, axis=(1, 2, 3))\n",
    "\n",
    "threshold_conv = np.mean(train_errors_conv) + np.std(train_errors_conv)\n",
    "print(f\"Threshold: {threshold_conv:.6f}\")\n",
    "\n",
    "test_reconstructed_orig = conv_autoencoder.predict(x_test_mnist)\n",
    "test_reconstructed_noisy = conv_autoencoder.predict(x_test_noisy)\n",
    "\n",
    "test_errors_orig = np.mean((x_test_mnist - test_reconstructed_orig) ** 2, axis=(1, 2, 3))\n",
    "test_errors_noisy = np.mean((x_test_noisy.numpy() - test_reconstructed_noisy) ** 2, axis=(1, 2, 3))\n",
    "\n",
    "y_true_orig = np.zeros(len(x_test_mnist))\n",
    "y_true_noisy = np.ones(len(x_test_noisy))\n",
    "\n",
    "y_pred_orig = (test_errors_orig > threshold_conv).astype(int)\n",
    "y_pred_noisy = (test_errors_noisy > threshold_conv).astype(int)\n",
    "\n",
    "acc_orig = np.mean(y_pred_orig == y_true_orig)\n",
    "acc_noisy = np.mean(y_pred_noisy == y_true_noisy)\n",
    "\n",
    "print(f\"Accuracy on original test images: {acc_orig:.4f}\")\n",
    "print(f\"Accuracy on noisy test images: {acc_noisy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85fc3c",
   "metadata": {},
   "source": [
    "### 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8018fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "fig, axes = plt.subplots(4, n_images, figsize=(15, 12))\n",
    "\n",
    "for i in range(n_images):\n",
    "    # Row 1\n",
    "    axes[0, i].imshow(x_test_mnist[i].squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Original', fontsize=12)\n",
    "    \n",
    "    # Row 2\n",
    "    axes[1, i].imshow(x_test_noisy[i].numpy().squeeze(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Noisy', fontsize=12)\n",
    "    \n",
    "    # Row 3\n",
    "    axes[2, i].imshow(test_reconstructed_orig[i].squeeze(), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_title('Reconstructed (Original)', fontsize=12)\n",
    "    \n",
    "    # Row 4\n",
    "    axes[3, i].imshow(test_reconstructed_noisy[i].squeeze(), cmap='gray')\n",
    "    axes[3, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[3, i].set_title('Reconstructed (Noisy)', fontsize=12)\n",
    "\n",
    "plt.suptitle('Standard Convolutional Autoencoder', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e2b3f",
   "metadata": {},
   "source": [
    "### 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoising_autoencoder = ConvAutoencoder()\n",
    "denoising_autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "x_train_noisy_dae = x_train_mnist + noise_factor * tf.random.normal(shape=x_train_mnist.shape)\n",
    "x_train_noisy_dae = tf.clip_by_value(x_train_noisy_dae, 0.0, 1.0)\n",
    "\n",
    "history_dae = denoising_autoencoder.fit(\n",
    "    x_train_noisy_dae, x_train_mnist,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_test_noisy, x_test_mnist),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_reconstructed_orig_dae = denoising_autoencoder.predict(x_test_mnist)\n",
    "test_reconstructed_noisy_dae = denoising_autoencoder.predict(x_test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf02558",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "fig, axes = plt.subplots(4, n_images, figsize=(15, 12))\n",
    "\n",
    "for i in range(n_images):\n",
    "    # Row 1\n",
    "    axes[0, i].imshow(x_test_mnist[i].squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Original', fontsize=12)\n",
    "    \n",
    "    # Row 2\n",
    "    axes[1, i].imshow(x_test_noisy[i].numpy().squeeze(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Noisy', fontsize=12)\n",
    "    \n",
    "    # Row 3\n",
    "    axes[2, i].imshow(test_reconstructed_orig_dae[i].squeeze(), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_title('Reconstructed (Original)', fontsize=12)\n",
    "    \n",
    "    # Row 47\n",
    "    axes[3, i].imshow(test_reconstructed_noisy_dae[i].squeeze(), cmap='gray')\n",
    "    axes[3, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[3, i].set_title('Reconstructed (Noisy)', fontsize=12)\n",
    "\n",
    "plt.suptitle('Denoising Convolutional Autoencoder', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
