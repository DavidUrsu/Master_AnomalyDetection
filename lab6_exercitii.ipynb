{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b59c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pyod.models.lof import LOF\n",
    "from scipy.io import loadmat\n",
    "\n",
    "np.random.seed(67)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115dc2d4",
   "metadata": {},
   "source": [
    "## Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9717a",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "with open('ca-AstroPh.txt', 'r') as f:\n",
    "    lines = f.readlines()[:1500]\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) >= 2:\n",
    "        node1, node2 = parts[0], parts[1]\n",
    "        if G.has_edge(node1, node2):\n",
    "            G[node1][node2]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(node1, node2, weight=1)\n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba169804",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e37c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = {}\n",
    "\n",
    "for node in G.nodes():\n",
    "    egonet = nx.ego_graph(G, node, radius=1)\n",
    "    \n",
    "    N_i = egonet.number_of_nodes() - 1\n",
    "    E_i = egonet.number_of_edges()\n",
    "    W_i = sum(d['weight'] for u, v, d in egonet.edges(data=True))\n",
    "    \n",
    "    if egonet.number_of_nodes() > 0:\n",
    "        adj_matrix = nx.to_numpy_array(egonet, weight='weight')\n",
    "        eigenvalues, _ = np.linalg.eigh(adj_matrix)\n",
    "        lambda_w_i = np.max(eigenvalues)\n",
    "    else:\n",
    "        lambda_w_i = 0\n",
    "    \n",
    "    node_features[node] = {\n",
    "        'N_i': N_i,\n",
    "        'E_i': E_i,\n",
    "        'W_i': W_i,\n",
    "        'lambda_w_i': lambda_w_i\n",
    "    }\n",
    "\n",
    "nx.set_node_attributes(G, node_features)\n",
    "\n",
    "print(f\"Features extracted for {len(node_features)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5144adb",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbc2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes())\n",
    "N_values = np.array([G.nodes[n]['N_i'] for n in nodes])\n",
    "E_values = np.array([G.nodes[n]['E_i'] for n in nodes])\n",
    "\n",
    "valid_mask = (N_values > 0) & (E_values > 0)\n",
    "valid_nodes = [nodes[i] for i in range(len(nodes)) if valid_mask[i]]\n",
    "N_valid = N_values[valid_mask]\n",
    "E_valid = E_values[valid_mask]\n",
    "\n",
    "log_N = np.log(N_valid).reshape(-1, 1)\n",
    "log_E = np.log(E_valid)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(log_N, log_E)\n",
    "\n",
    "theta = reg.coef_[0]\n",
    "log_C = reg.intercept_\n",
    "C = np.exp(log_C)\n",
    "\n",
    "print(f\"Power-law parameters: C = {C:.4f}, theta = {theta:.4f}\")\n",
    "\n",
    "E_pred = C * (N_valid ** theta)\n",
    "\n",
    "anomaly_scores = {}\n",
    "for i, node in enumerate(valid_nodes):\n",
    "    y_i = E_valid[i]\n",
    "    y_pred = E_pred[i]\n",
    "    \n",
    "    score = (max(y_i, y_pred) / (min(y_i, y_pred) + 1e-8)) * np.log(abs(y_i - y_pred) + 1)\n",
    "    anomaly_scores[node] = score\n",
    "\n",
    "for node in nodes:\n",
    "    if node not in anomaly_scores:\n",
    "        anomaly_scores[node] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af113312",
   "metadata": {},
   "source": [
    "### 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ac41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_nodes = sorted(anomaly_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "top_10_anomalies = set([n for n, s in sorted_nodes[:10]])\n",
    "\n",
    "node_colors = ['red' if n in top_10_anomalies else 'lightblue' for n in G.nodes()]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "nx.draw(G, node_color=node_colors, node_size=50, with_labels=False)\n",
    "plt.title('Graph with Top 10 Anomalies (OddBall - E vs N)')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 anomalous nodes:\")\n",
    "for node, score in sorted_nodes[:10]:\n",
    "    print(f\"  Node {node}: score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c062cd1",
   "metadata": {},
   "source": [
    "### 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af9f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_lof = np.column_stack([N_valid, E_valid])\n",
    "\n",
    "lof = LOF()\n",
    "lof.fit(features_for_lof)\n",
    "lof_scores = lof.decision_scores_\n",
    "\n",
    "oddball_scores = np.array([anomaly_scores[n] for n in valid_nodes])\n",
    "oddball_normalized = (oddball_scores - oddball_scores.min()) / (oddball_scores.max() - oddball_scores.min() + 1e-8)\n",
    "lof_normalized = (lof_scores - lof_scores.min()) / (lof_scores.max() - lof_scores.min() + 1e-8)\n",
    "\n",
    "combined_scores = oddball_normalized + lof_normalized\n",
    "\n",
    "combined_anomaly_scores = {}\n",
    "for i, node in enumerate(valid_nodes):\n",
    "    combined_anomaly_scores[node] = combined_scores[i]\n",
    "\n",
    "for node in nodes:\n",
    "    if node not in combined_anomaly_scores:\n",
    "        combined_anomaly_scores[node] = 0\n",
    "\n",
    "sorted_combined = sorted(combined_anomaly_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "top_10_combined = set([n for n, s in sorted_combined[:10]])\n",
    "\n",
    "node_colors_combined = ['red' if n in top_10_combined else 'lightblue' for n in G.nodes()]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "nx.draw(G, node_color=node_colors_combined, node_size=50, with_labels=False)\n",
    "plt.title('Graph with Top 10 Anomalies (OddBall + LOF)')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 anomalous nodes (combined score):\")\n",
    "for node, score in sorted_combined[:10]:\n",
    "    print(f\"  Node {node}: score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d4500",
   "metadata": {},
   "source": [
    "## Ex 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64998f2a",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_regular = nx.random_regular_graph(3, 100, seed=42)\n",
    "\n",
    "G_caveman = nx.connected_caveman_graph(10, 20)\n",
    "\n",
    "G_merged = nx.union(G_regular, G_caveman, rename=('R', 'C'))\n",
    "\n",
    "regular_nodes = [n for n in G_merged.nodes() if str(n).startswith('R')]\n",
    "caveman_nodes = [n for n in G_merged.nodes() if str(n).startswith('C')]\n",
    "\n",
    "for _ in range(10):\n",
    "    n1 = np.random.choice(regular_nodes)\n",
    "    n2 = np.random.choice(caveman_nodes)\n",
    "    G_merged.add_edge(n1, n2)\n",
    "\n",
    "print(f\"Merged graph - Nodes: {G_merged.number_of_nodes()}, Edges: {G_merged.number_of_edges()}\")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "nx.draw(G_merged, node_size=30, with_labels=False)\n",
    "plt.title('Merged Graph (Regular + Caveman)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfd07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_merged = {}\n",
    "\n",
    "for node in G_merged.nodes():\n",
    "    egonet = nx.ego_graph(G_merged, node, radius=1)\n",
    "    N_i = egonet.number_of_nodes() - 1\n",
    "    E_i = egonet.number_of_edges()\n",
    "    node_features_merged[node] = {'N_i': N_i, 'E_i': E_i}\n",
    "\n",
    "nodes_m = list(G_merged.nodes())\n",
    "N_m = np.array([node_features_merged[n]['N_i'] for n in nodes_m])\n",
    "E_m = np.array([node_features_merged[n]['E_i'] for n in nodes_m])\n",
    "\n",
    "valid_mask_m = (N_m > 0) & (E_m > 0)\n",
    "valid_nodes_m = [nodes_m[i] for i in range(len(nodes_m)) if valid_mask_m[i]]\n",
    "N_valid_m = N_m[valid_mask_m]\n",
    "E_valid_m = E_m[valid_mask_m]\n",
    "\n",
    "log_N_m = np.log(N_valid_m).reshape(-1, 1)\n",
    "log_E_m = np.log(E_valid_m)\n",
    "\n",
    "reg_m = LinearRegression()\n",
    "reg_m.fit(log_N_m, log_E_m)\n",
    "theta_m = reg_m.coef_[0]\n",
    "C_m = np.exp(reg_m.intercept_)\n",
    "\n",
    "E_pred_m = C_m * (N_valid_m ** theta_m)\n",
    "\n",
    "scores_m = {}\n",
    "for i, node in enumerate(valid_nodes_m):\n",
    "    y_i = E_valid_m[i]\n",
    "    y_pred = E_pred_m[i]\n",
    "    score = (max(y_i, y_pred) / (min(y_i, y_pred) + 1e-8)) * np.log(abs(y_i - y_pred) + 1)\n",
    "    scores_m[node] = score\n",
    "\n",
    "for node in nodes_m:\n",
    "    if node not in scores_m:\n",
    "        scores_m[node] = 0\n",
    "\n",
    "sorted_m = sorted(scores_m.items(), key=lambda x: x[1], reverse=True)\n",
    "top_10_clique = set([n for n, s in sorted_m[:10]])\n",
    "\n",
    "colors_m = ['red' if n in top_10_clique else 'lightblue' for n in G_merged.nodes()]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "nx.draw(G_merged, node_color=colors_m, node_size=30, with_labels=False)\n",
    "plt.title('Clique Detection - Top 10 Anomalies')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 nodes (likely part of cliques):\")\n",
    "for node, score in sorted_m[:10]:\n",
    "    print(f\"  Node {node}: score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9502e",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b648b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = nx.random_regular_graph(3, 100, seed=42)\n",
    "G2 = nx.random_regular_graph(5, 100, seed=43)\n",
    "\n",
    "G_heavy = nx.union(G1, G2, rename=('A', 'B'))\n",
    "\n",
    "for edge in list(G_heavy.edges()):\n",
    "    G_heavy.add_edge(edge[0], edge[1], weight=1)\n",
    "\n",
    "A_nodes = [n for n in G_heavy.nodes() if str(n).startswith('A')]\n",
    "B_nodes = [n for n in G_heavy.nodes() if str(n).startswith('B')]\n",
    "\n",
    "for _ in range(5):\n",
    "    n1 = np.random.choice(A_nodes)\n",
    "    n2 = np.random.choice(B_nodes)\n",
    "    G_heavy.add_edge(n1, n2, weight=1)\n",
    "\n",
    "random_nodes = np.random.choice(list(G_heavy.nodes()), 2, replace=False)\n",
    "heavy_nodes = set(random_nodes)\n",
    "\n",
    "for node in random_nodes:\n",
    "    egonet = nx.ego_graph(G_heavy, node, radius=1)\n",
    "    for u, v in egonet.edges():\n",
    "        G_heavy[u][v]['weight'] += 10\n",
    "\n",
    "print(f\"Heavy vicinity nodes: {random_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_heavy = {}\n",
    "\n",
    "for node in G_heavy.nodes():\n",
    "    egonet = nx.ego_graph(G_heavy, node, radius=1)\n",
    "    E_i = egonet.number_of_edges()\n",
    "    W_i = sum(d['weight'] for u, v, d in egonet.edges(data=True))\n",
    "    node_features_heavy[node] = {'E_i': E_i, 'W_i': W_i}\n",
    "\n",
    "nodes_h = list(G_heavy.nodes())\n",
    "E_h = np.array([node_features_heavy[n]['E_i'] for n in nodes_h])\n",
    "W_h = np.array([node_features_heavy[n]['W_i'] for n in nodes_h])\n",
    "\n",
    "valid_mask_h = (E_h > 0) & (W_h > 0)\n",
    "valid_nodes_h = [nodes_h[i] for i in range(len(nodes_h)) if valid_mask_h[i]]\n",
    "E_valid_h = E_h[valid_mask_h]\n",
    "W_valid_h = W_h[valid_mask_h]\n",
    "\n",
    "log_E_h = np.log(E_valid_h).reshape(-1, 1)\n",
    "log_W_h = np.log(W_valid_h)\n",
    "\n",
    "reg_h = LinearRegression()\n",
    "reg_h.fit(log_E_h, log_W_h)\n",
    "theta_h = reg_h.coef_[0]\n",
    "C_h = np.exp(reg_h.intercept_)\n",
    "\n",
    "W_pred_h = C_h * (E_valid_h ** theta_h)\n",
    "\n",
    "scores_h = {}\n",
    "for i, node in enumerate(valid_nodes_h):\n",
    "    y_i = W_valid_h[i]\n",
    "    y_pred = W_pred_h[i]\n",
    "    score = (max(y_i, y_pred) / (min(y_i, y_pred) + 1e-8)) * np.log(abs(y_i - y_pred) + 1)\n",
    "    scores_h[node] = score\n",
    "\n",
    "for node in nodes_h:\n",
    "    if node not in scores_h:\n",
    "        scores_h[node] = 0\n",
    "\n",
    "sorted_h = sorted(scores_h.items(), key=lambda x: x[1], reverse=True)\n",
    "top_4_heavy = set([n for n, s in sorted_h[:4]])\n",
    "\n",
    "colors_h = ['red' if n in top_4_heavy else 'lightblue' for n in G_heavy.nodes()]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "nx.draw(G_heavy, node_color=colors_h, node_size=30, with_labels=False)\n",
    "plt.title('Heavy Vicinity Detection - Top 4 Anomalies')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 4 nodes (heavy vicinity):\")\n",
    "for node, score in sorted_h[:4]:\n",
    "    print(f\"  Node {node}: score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b93e59",
   "metadata": {},
   "source": [
    "## Ex 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfd8bd",
   "metadata": {},
   "source": [
    "### 3.1-3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a39b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_data = loadmat('ACM.mat')\n",
    "\n",
    "X = mat_data['Attributes']\n",
    "if hasattr(X, 'toarray'):\n",
    "    X = X.toarray()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "A_sparse = mat_data['Network']\n",
    "edge_index, edge_weight = from_scipy_sparse_matrix(A_sparse)\n",
    "\n",
    "A = torch.tensor(A_sparse.toarray(), dtype=torch.float32)\n",
    "\n",
    "y = mat_data['Label'].ravel()\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "print(f\"Node attributes shape: {X.shape}\")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")\n",
    "print(f\"Adjacency matrix shape: {A.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Number of anomalies: {int(y.sum())} ({y.sum()/len(y)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c93595",
   "metadata": {},
   "source": [
    "### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttributeDecoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(AttributeDecoder, self).__init__()\n",
    "        self.conv1 = GCNConv(64, 128)\n",
    "        self.conv2 = GCNConv(128, output_dim)\n",
    "    \n",
    "    def forward(self, z, edge_index):\n",
    "        z = self.conv1(z, edge_index)\n",
    "        z = F.relu(z)\n",
    "        z = self.conv2(z, edge_index)\n",
    "        z = F.relu(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class StructureDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StructureDecoder, self).__init__()\n",
    "        self.conv = GCNConv(64, 64)\n",
    "    \n",
    "    def forward(self, z, edge_index):\n",
    "        z = self.conv(z, edge_index)\n",
    "        z = F.relu(z)\n",
    "        A_hat = z @ z.T\n",
    "        return A_hat\n",
    "\n",
    "\n",
    "class GraphAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GraphAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(input_dim)\n",
    "        self.attr_decoder = AttributeDecoder(input_dim)\n",
    "        self.struct_decoder = StructureDecoder()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encoder(x, edge_index)\n",
    "        x_hat = self.attr_decoder(z, edge_index)\n",
    "        a_hat = self.struct_decoder(z, edge_index)\n",
    "        return x_hat, a_hat, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2130e",
   "metadata": {},
   "source": [
    "### 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae_loss(X, X_hat, A, A_hat, alpha=0.8):\n",
    "    attr_loss = torch.norm(X - X_hat, p='fro') ** 2\n",
    "    struct_loss = torch.norm(A - A_hat, p='fro') ** 2\n",
    "    return alpha * attr_loss + (1 - alpha) * struct_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a12b9",
   "metadata": {},
   "source": [
    "### 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "model = GraphAutoencoder(input_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.004)\n",
    "\n",
    "alpha = 0.8\n",
    "n_epochs = 100\n",
    "\n",
    "losses = []\n",
    "aucs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    X_hat, A_hat, z = model(X, edge_index)\n",
    "    \n",
    "    loss = gae_loss(X, X_hat, A, A_hat, alpha)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_hat, A_hat, z = model(X, edge_index)\n",
    "            \n",
    "            attr_error = torch.sum((X - X_hat) ** 2, dim=1)\n",
    "            struct_error = torch.sum((A - A_hat) ** 2, dim=1)\n",
    "            scores = alpha * attr_error + (1 - alpha) * struct_error\n",
    "            \n",
    "            auc = roc_auc_score(y.numpy(), scores.numpy())\n",
    "            aucs.append(auc)\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {loss.item():.4f} - ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ba779",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(5, n_epochs+1, 5), aucs)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('ROC AUC')\n",
    "axes[1].set_title('ROC AUC Score')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final ROC AUC: {aucs[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
