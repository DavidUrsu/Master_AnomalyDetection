{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a21872",
   "metadata": {},
   "source": [
    "## Ex 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f41e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279a6db",
   "metadata": {},
   "source": [
    "### 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50919a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _ = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=2,\n",
    "    centers=[[0, 0]],\n",
    "    cluster_std=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], alpha=0.6, s=25, c='blue')\n",
    "plt.title('Training Data: Standard Normal Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680b5dd",
   "metadata": {},
   "source": [
    "### 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_projections = 5\n",
    "n_features = 2\n",
    "\n",
    "mean = np.zeros(n_features)\n",
    "cov = np.eye(n_features)\n",
    "\n",
    "projection_vectors = np.random.multivariate_normal(\n",
    "    mean=mean,\n",
    "    cov=cov,\n",
    "    size=n_projections\n",
    ")\n",
    "\n",
    "# Normalize to unit length\n",
    "projection_vectors = projection_vectors / np.linalg.norm(projection_vectors, axis=1, keepdims=True)\n",
    "\n",
    "print(n_projections)\n",
    "print(projection_vectors)\n",
    "\n",
    "# Visualize projection vectors\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], alpha=0.3, s=20, c='lightblue', label='Training data')\n",
    "\n",
    "# Draw projection vectors as arrows from origin\n",
    "colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "for i, (vec, color) in enumerate(zip(projection_vectors, colors)):\n",
    "    plt.arrow(0, 0, vec[0]*2, vec[1]*2, \n",
    "             head_width=0.15, head_length=0.1, \n",
    "             fc=color, ec=color, linewidth=2, \n",
    "             label=f'Projection {i+1}')\n",
    "\n",
    "plt.title('Training Data with Projection Vectors', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.axis('equal')\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b6063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_histograms(X, projection_vectors, n_bins=10, range_margin=1.5):\n",
    "    histograms = []\n",
    "    probabilities = []\n",
    "    \n",
    "    for i, vec in enumerate(projection_vectors):\n",
    "        # Project data onto this vector\n",
    "        projections = X @ vec\n",
    "        \n",
    "        # Compute range for histogram\n",
    "        data_min, data_max = projections.min(), projections.max()\n",
    "        data_range = data_max - data_min\n",
    "        hist_range = (data_min - range_margin * data_range, \n",
    "                     data_max + range_margin * data_range)\n",
    "        \n",
    "        # Build histogram\n",
    "        counts, bin_edges = np.histogram(projections, bins=n_bins, range=hist_range)\n",
    "        \n",
    "        # Convert counts to probabilities\n",
    "        # Add small epsilon to avoid division by zero\n",
    "        probs = (counts + 1e-10) / (counts.sum() + 1e-10 * n_bins)\n",
    "        \n",
    "        histograms.append((counts, bin_edges))\n",
    "        probabilities.append(probs)\n",
    "        \n",
    "    return histograms, probabilities\n",
    "\n",
    "n_bins = 10\n",
    "histograms, probabilities = build_histograms(X_train, projection_vectors, n_bins=n_bins)\n",
    "\n",
    "print(probabilities[0])\n",
    "print(f\"Sum of probabilities: {probabilities[0].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize histograms for each projection\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors_hist = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "\n",
    "for i, (vec, (counts, bin_edges), probs, color) in enumerate(zip(\n",
    "    projection_vectors, histograms, probabilities, colors_hist)):\n",
    "    \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot histogram\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    ax.bar(bin_centers, probs, width=(bin_edges[1]-bin_edges[0])*0.9, \n",
    "           color=color, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax.set_title(f'Projection {i+1}: [{vec[0]:.3f}, {vec[1]:.3f}]', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Projected Value')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'Histograms for Each Projection ({n_bins} bins)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores(X_test, projection_vectors, histograms, probabilities):\n",
    "    n_samples = X_test.shape[0]\n",
    "    n_projections = len(projection_vectors)\n",
    "    \n",
    "    sample_probabilities = np.zeros((n_samples, n_projections))\n",
    "    \n",
    "    for i, vec in enumerate(projection_vectors):\n",
    "        projections = X_test @ vec\n",
    "        \n",
    "        _, bin_edges = histograms[i]\n",
    "        probs = probabilities[i]\n",
    "        \n",
    "        bin_indices = np.digitize(projections, bin_edges) - 1\n",
    "        \n",
    "        bin_indices = np.clip(bin_indices, 0, len(probs) - 1)\n",
    "        \n",
    "        sample_probabilities[:, i] = probs[bin_indices]\n",
    "    \n",
    "    # Compute mean probability across all projections\n",
    "    mean_probabilities = sample_probabilities.mean(axis=1)\n",
    "    \n",
    "    # Convert to anomaly scores\n",
    "    anomaly_scores = -np.log(mean_probabilities + 1e-10)\n",
    "    \n",
    "    return anomaly_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebd9895",
   "metadata": {},
   "source": [
    "### 2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 500\n",
    "X_test = np.random.uniform(-3, 3, size=(n_test, 2))\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_test.min(), X_test.max())\n",
    "\n",
    "# Compute anomaly scores\n",
    "anomaly_scores = compute_anomaly_scores(X_test, projection_vectors, histograms, probabilities)\n",
    "\n",
    "print(f\"\\nAnomaly scores computed:\")\n",
    "print(f\"Min score: {anomaly_scores.min():.4f}\")\n",
    "print(f\"Max score: {anomaly_scores.max():.4f}\")\n",
    "print(f\"Mean score: {anomaly_scores.mean():.4f}\")\n",
    "print(f\"Std score: {anomaly_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36563483",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot 1: Training data vs Test data\n",
    "ax = axes[0]\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], alpha=0.4, s=20, c='blue', label='Training (Normal)')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], alpha=0.4, s=20, c='red', label='Test (Uniform)')\n",
    "ax.set_title('Training vs Test Data Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')\n",
    "\n",
    "# Plot 2: Test data colored by anomaly score\n",
    "ax = axes[1]\n",
    "scatter = ax.scatter(X_test[:, 0], X_test[:, 1], \n",
    "                     c=anomaly_scores, cmap='RdYlBu_r', \n",
    "                     s=50, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "plt.colorbar(scatter, ax=ax, label='Anomaly Score')\n",
    "\n",
    "# Overlay training data distribution (faintly)\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], alpha=0.1, s=10, c='blue', marker='.')\n",
    "\n",
    "ax.set_title(f'Test Data with Anomaly Scores ({n_bins} bins)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f40a4",
   "metadata": {},
   "source": [
    "### 2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of bins\n",
    "bin_values = [5, 10, 20, 30, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, n_bins_test in enumerate(bin_values):\n",
    "    # Build histograms with different bin count\n",
    "    histograms_test, probabilities_test = build_histograms(\n",
    "        X_train, projection_vectors, n_bins=n_bins_test\n",
    "    )\n",
    "    \n",
    "    # Compute anomaly scores\n",
    "    anomaly_scores_test = compute_anomaly_scores(\n",
    "        X_test, projection_vectors, histograms_test, probabilities_test\n",
    "    )\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    scatter = ax.scatter(X_test[:, 0], X_test[:, 1], \n",
    "                        c=anomaly_scores_test, cmap='RdYlBu_r', \n",
    "                        s=40, alpha=0.7, edgecolors='black', linewidth=0.3)\n",
    "    \n",
    "    # Overlay training data\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], alpha=0.1, s=5, c='blue', marker='.')\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Anomaly Score', fontsize=9)\n",
    "    \n",
    "    ax.set_title(f'{n_bins_test} bins\\nScore range: [{anomaly_scores_test.min():.2f}, {anomaly_scores_test.max():.2f}]', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1', fontsize=10)\n",
    "    ax.set_ylabel('Feature 2', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axis('equal')\n",
    "\n",
    "plt.suptitle('Effect of Number of Bins on Anomaly Score Map', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(f\"{'Bins':<10} {'Min Score':<12} {'Max Score':<12} {'Mean Score':<12} {'Std Score':<12}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for n_bins_test in bin_values:\n",
    "    histograms_test, probabilities_test = build_histograms(\n",
    "        X_train, projection_vectors, n_bins=n_bins_test\n",
    "    )\n",
    "    anomaly_scores_test = compute_anomaly_scores(\n",
    "        X_test, projection_vectors, histograms_test, probabilities_test\n",
    "    )\n",
    "    \n",
    "    print(f\"{n_bins_test:<10} {anomaly_scores_test.min():<12.4f} \"\n",
    "          f\"{anomaly_scores_test.max():<12.4f} {anomaly_scores_test.mean():<12.4f} \"\n",
    "          f\"{anomaly_scores_test.std():<12.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede8091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01fdec3f",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.loda import LODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2 clusters with specified centers\n",
    "X_train_ex2, y_train_ex2 = make_blobs(\n",
    "    n_samples=[500, 500],\n",
    "    n_features=2,\n",
    "    centers=[[10, 0], [0, 10]],  # Two cluster centers\n",
    "    cluster_std=1.0,  # Standard deviation = 1\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(X_train_ex2.shape)\n",
    "print(f\"Cluster 1 center: [10, 0]\")\n",
    "print(f\"Cluster 2 center: [0, 10]\")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_train_ex2[y_train_ex2==0, 0], X_train_ex2[y_train_ex2==0, 1], \n",
    "            alpha=0.6, s=30, c='blue', label='Cluster 1 (10, 0)')\n",
    "plt.scatter(X_train_ex2[y_train_ex2==1, 0], X_train_ex2[y_train_ex2==1, 1], \n",
    "            alpha=0.6, s=30, c='green', label='Cluster 2 (0, 10)')\n",
    "plt.title('Training Data: Two Clusters', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e04b1",
   "metadata": {},
   "source": [
    "### 2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_ex2 = IForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.02,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "iforest_ex2.fit(X_train_ex2)\n",
    "\n",
    "n_test_ex2 = 1000\n",
    "X_test_ex2 = np.random.uniform(-10, 20, size=(n_test_ex2, 2))\n",
    "\n",
    "print(X_test_ex2.shape)\n",
    "print(X_test_ex2.min(), X_test_ex2.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a300400",
   "metadata": {},
   "source": [
    "### 2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomaly scores for test data\n",
    "scores_iforest = iforest_ex2.decision_function(X_test_ex2)\n",
    "\n",
    "print(f\"IForest Anomaly Scores:\")\n",
    "print(f\"Min: {scores_iforest.min():.4f}\")\n",
    "print(f\"Max: {scores_iforest.max():.4f}\")\n",
    "print(f\"Mean: {scores_iforest.mean():.4f}\")\n",
    "\n",
    "# colormap\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(X_test_ex2[:, 0], X_test_ex2[:, 1], \n",
    "                     c=scores_iforest, cmap='RdYlBu_r', \n",
    "                     s=30, alpha=0.7, edgecolors='black', linewidth=0.3)\n",
    "\n",
    "# Overlay training data\n",
    "plt.scatter(X_train_ex2[:, 0], X_train_ex2[:, 1], \n",
    "           alpha=0.2, s=10, c='gray', marker='.')\n",
    "\n",
    "plt.colorbar(scatter, label='Anomaly Score')\n",
    "plt.title('IForest: Anomaly Scores', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315e29d",
   "metadata": {},
   "source": [
    "### 2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LODA model\n",
    "loda_ex2 = LODA(contamination=0.02, n_bins=10, n_random_cuts=100)\n",
    "loda_ex2.fit(X_train_ex2)\n",
    "scores_loda = loda_ex2.decision_function(X_test_ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf653712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"it works\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"--quiet\"])\n",
    "        print(\"ready\")\n",
    "        \n",
    "        import torch\n",
    "        print(f\"   Version: {torch.__version__}\")\n",
    "    except Exception as e:\n",
    "        print(\"rip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.dif import DIF\n",
    "    \n",
    "dif_ex2 = DIF(contamination=0.02, hidden_neurons=[64, 32], random_state=42)\n",
    "dif_ex2.fit(X_train_ex2)\n",
    "scores_dif = dif_ex2.decision_function(X_test_ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
    "models_data = [\n",
    "    ('IForest', scores_iforest),\n",
    "    ('DIF (Deep IForest)', scores_dif),\n",
    "    ('LODA', scores_loda)\n",
    "]\n",
    "\n",
    "for idx, (name, scores) in enumerate(models_data):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot test data with scores\n",
    "    scatter = ax.scatter(X_test_ex2[:, 0], X_test_ex2[:, 1], \n",
    "                        c=scores, cmap='RdYlBu_r', \n",
    "                        s=30, alpha=0.7, edgecolors='black', linewidth=0.3)\n",
    "    \n",
    "    # Overlay training data\n",
    "    ax.scatter(X_train_ex2[:, 0], X_train_ex2[:, 1], \n",
    "              alpha=0.2, s=10, c='gray', marker='.')\n",
    "    \n",
    "    plt.colorbar(scatter, ax=ax, label='Anomaly Score')\n",
    "    ax.set_title(f'{name}\\nScore range: [{scores.min():.2f}, {scores.max():.2f}]', \n",
    "                fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axis('equal')\n",
    "\n",
    "plt.suptitle('Comparison: IForest vs DIF vs LODA (2D)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b3ac2",
   "metadata": {},
   "source": [
    "### 2.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c114f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_configs = [5, 10, 20, 50]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, n_bins_config in enumerate(bin_configs):\n",
    "    loda_test = LODA(contamination=0.02, n_bins=n_bins_config, n_random_cuts=100)\n",
    "    loda_test.fit(X_train_ex2)\n",
    "    scores_test = loda_test.decision_function(X_test_ex2)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    scatter = ax.scatter(X_test_ex2[:, 0], X_test_ex2[:, 1], \n",
    "                        c=scores_test, cmap='RdYlBu_r', \n",
    "                        s=30, alpha=0.7, edgecolors='black', linewidth=0.3)\n",
    "    \n",
    "    # Overlay training data\n",
    "    ax.scatter(X_train_ex2[:, 0], X_train_ex2[:, 1], \n",
    "              alpha=0.2, s=10, c='gray', marker='.')\n",
    "    \n",
    "    plt.colorbar(scatter, ax=ax, label='Anomaly Score')\n",
    "    ax.set_title(f'LODA: {n_bins_config} bins\\nScore range: [{scores_test.min():.2f}, {scores_test.max():.2f}]', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axis('equal')\n",
    "\n",
    "plt.suptitle('LODA: Effect of Number of Bins', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# LODA uses random projections in many directions, averaging over multiple 1D histograms.\n",
    "# Few bins => coarse boundaries; many bins => finer detail but noisier.\n",
    "# Unlike IForest, LODA has no axis-parallel artifacts due to random projection directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_configs = [\n",
    "    [32],          \n",
    "    [64, 32],      \n",
    "    [128, 64, 32], \n",
    "    [64, 64] \n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, hidden_neurons in enumerate(hidden_configs):\n",
    "    dif_test = DIF(contamination=0.02, hidden_neurons=hidden_neurons, random_state=42)\n",
    "    dif_test.fit(X_train_ex2)\n",
    "    scores_test = dif_test.decision_function(X_test_ex2)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    scatter = ax.scatter(X_test_ex2[:, 0], X_test_ex2[:, 1], \n",
    "                        c=scores_test, cmap='RdYlBu_r', \n",
    "                        s=30, alpha=0.7, edgecolors='black', linewidth=0.3)\n",
    "    \n",
    "    ax.scatter(X_train_ex2[:, 0], X_train_ex2[:, 1], \n",
    "              alpha=0.2, s=10, c='gray', marker='.')\n",
    "    \n",
    "    plt.colorbar(scatter, ax=ax, label='Anomaly Score')\n",
    "    ax.set_title(f'DIF: hidden_neurons={hidden_neurons}\\nScore range: [{scores_test.min():.2f}, {scores_test.max():.2f}]', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "plt.suptitle('DIF: Effect of Hidden Layer Configuration', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f5c3b",
   "metadata": {},
   "source": [
    "### 2.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a604e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 3D training data\n",
    "X_train_3d, y_train_3d = make_blobs(\n",
    "    n_samples=[500, 500],\n",
    "    n_features=3,\n",
    "    centers=[[0, 10, 0], [10, 0, 10]],\n",
    "    cluster_std=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(X_train_3d.shape)\n",
    "\n",
    "# Visualize 3D training data\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_train_3d[y_train_3d==0, 0], \n",
    "          X_train_3d[y_train_3d==0, 1], \n",
    "          X_train_3d[y_train_3d==0, 2],\n",
    "          alpha=0.6, s=30, c='blue', label='Cluster 1 (0, 10, 0)')\n",
    "ax.scatter(X_train_3d[y_train_3d==1, 0], \n",
    "          X_train_3d[y_train_3d==1, 1], \n",
    "          X_train_3d[y_train_3d==1, 2],\n",
    "          alpha=0.6, s=30, c='green', label='Cluster 2 (10, 0, 10)')\n",
    "\n",
    "ax.set_title('3D Training Data: Two Clusters', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_zlabel('Feature 3')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_3d = 1000\n",
    "X_test_3d = np.random.uniform(-10, 20, size=(n_test_3d, 3))\n",
    "\n",
    "print(X_test_3d.shape)\n",
    "\n",
    "# Train models on 3D data\n",
    "iforest_3d = IForest(n_estimators=100, contamination=0.02, random_state=42)\n",
    "iforest_3d.fit(X_train_3d)\n",
    "scores_iforest_3d = iforest_3d.decision_function(X_test_3d)\n",
    "\n",
    "loda_3d = LODA(contamination=0.02, n_bins=10, n_random_cuts=100)\n",
    "loda_3d.fit(X_train_3d)\n",
    "scores_loda_3d = loda_3d.decision_function(X_test_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595360e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dif_3d = DIF(contamination=0.02, hidden_neurons=[64, 32], random_state=42)\n",
    "dif_3d.fit(X_train_3d)\n",
    "scores_dif_3d = dif_3d.decision_function(X_test_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdc9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(22, 7))\n",
    "models_3d = [\n",
    "    ('IForest', scores_iforest_3d),\n",
    "    ('DIF', scores_dif_3d),\n",
    "    ('LODA', scores_loda_3d)\n",
    "]\n",
    "n_plots = 3\n",
    "\n",
    "for idx, (name, scores) in enumerate(models_3d):\n",
    "    ax = fig.add_subplot(1, n_plots, idx+1, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(X_test_3d[:, 0], X_test_3d[:, 1], X_test_3d[:, 2],\n",
    "                        c=scores, cmap='RdYlBu_r', \n",
    "                        s=20, alpha=0.6, edgecolors='black', linewidth=0.2)\n",
    "    \n",
    "    # Overlay training data\n",
    "    ax.scatter(X_train_3d[:, 0], X_train_3d[:, 1], X_train_3d[:, 2],\n",
    "              alpha=0.1, s=5, c='gray', marker='.')\n",
    "    \n",
    "    plt.colorbar(scatter, ax=ax, label='Anomaly Score', shrink=0.6)\n",
    "    ax.set_title(f'{name} (3D)\\nScore range: [{scores.min():.2f}, {scores.max():.2f}]', \n",
    "                fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Feature 3')\n",
    "\n",
    "plt.suptitle('3D Comparison: IForest vs DIF vs LODA', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adc8ce",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f55b5b",
   "metadata": {},
   "source": [
    "### 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "mat_data = scipy.io.loadmat('shuttle.mat')\n",
    "\n",
    "# Extract features and labels\n",
    "X = mat_data['X']\n",
    "y = mat_data['y'].ravel()  # Flatten to 1D array\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Total samples: {X.shape[0]}\")\n",
    "print(f\"\\n  Normal (0): {(y == 0).sum()} samples ({(y == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Anomaly (1): {(y == 1).sum()} samples ({(y == 1).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shuttle, X_test_shuttle, y_train_shuttle, y_test_shuttle = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_shuttle_scaled = scaler.fit_transform(X_train_shuttle)\n",
    "X_test_shuttle_scaled = scaler.transform(X_test_shuttle)\n",
    "\n",
    "print(f\"Training set: {X_train_shuttle_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_shuttle_scaled.shape}\")\n",
    "print(f\"\\nTraining labels distribution:\")\n",
    "print(f\"  Normal: {(y_train_shuttle == 0).sum()}\")\n",
    "print(f\"  Anomaly: {(y_train_shuttle == 1).sum()}\")\n",
    "print(f\"\\nTest labels distribution:\")\n",
    "print(f\"  Normal: {(y_test_shuttle == 0).sum()}\")\n",
    "print(f\"  Anomaly: {(y_test_shuttle == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d9757",
   "metadata": {},
   "source": [
    "### 2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009eef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score\n",
    "\n",
    "# Compute contamination ratio from training data\n",
    "contamination_ratio = (y_train_shuttle == 1).sum() / len(y_train_shuttle)\n",
    "print(f\"Contamination ratio: {contamination_ratio:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Single Train-Test Split Evaluation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# IForest\n",
    "iforest_shuttle = IForest(n_estimators=100, contamination=contamination_ratio, random_state=42)\n",
    "iforest_shuttle.fit(X_train_shuttle_scaled)\n",
    "y_scores_iforest = iforest_shuttle.decision_function(X_test_shuttle_scaled)\n",
    "y_pred_iforest = iforest_shuttle.predict(X_test_shuttle_scaled)\n",
    "\n",
    "ba_iforest = balanced_accuracy_score(y_test_shuttle, y_pred_iforest)\n",
    "auc_iforest = roc_auc_score(y_test_shuttle, y_scores_iforest)\n",
    "\n",
    "print(f\"\\n IForest:\")\n",
    "print(f\"   Balanced Accuracy: {ba_iforest:.4f}\")\n",
    "print(f\"   ROC AUC: {auc_iforest:.4f}\")\n",
    "\n",
    "# LODA\n",
    "loda_shuttle = LODA(contamination=contamination_ratio, n_bins=10, n_random_cuts=100)\n",
    "loda_shuttle.fit(X_train_shuttle_scaled)\n",
    "y_scores_loda = loda_shuttle.decision_function(X_test_shuttle_scaled)\n",
    "y_pred_loda = loda_shuttle.predict(X_test_shuttle_scaled)\n",
    "\n",
    "ba_loda = balanced_accuracy_score(y_test_shuttle, y_pred_loda)\n",
    "auc_loda = roc_auc_score(y_test_shuttle, y_scores_loda)\n",
    "\n",
    "print(f\"\\n LODA:\")\n",
    "print(f\"   Balanced Accuracy: {ba_loda:.4f}\")\n",
    "print(f\"   ROC AUC: {auc_loda:.4f}\")\n",
    "\n",
    "# DIF\n",
    "dif_shuttle = DIF(contamination=contamination_ratio, hidden_neurons=[64, 32], random_state=42)\n",
    "dif_shuttle.fit(X_train_shuttle_scaled)\n",
    "y_scores_dif = dif_shuttle.decision_function(X_test_shuttle_scaled)\n",
    "y_pred_dif = dif_shuttle.predict(X_test_shuttle_scaled)\n",
    "\n",
    "ba_dif = balanced_accuracy_score(y_test_shuttle, y_pred_dif)\n",
    "auc_dif = roc_auc_score(y_test_shuttle, y_scores_dif)\n",
    "\n",
    "print(f\"\\n DIF:\")\n",
    "print(f\"   Balanced Accuracy: {ba_dif:.4f}\")\n",
    "print(f\"   ROC AUC: {auc_dif:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b534ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10 different train-test splits and compute mean metrics\n",
    "# n_splits = 2\n",
    "n_splits = 10\n",
    "# random_seeds = [42, 123]\n",
    "random_seeds = [42, 123, 456, 789, 1011, 1213, 1415, 1617, 1819, 2021]\n",
    "\n",
    "results = {\n",
    "    'IForest': {'BA': [], 'AUC': []},\n",
    "    'LODA': {'BA': [], 'AUC': []},\n",
    "    'DIF': {'BA': [], 'AUC': []}\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Evaluating over {n_splits} different train-test splits...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, seed in enumerate(random_seeds):\n",
    "    print(f\"\\nSplit {i+1}/{n_splits} (seed={seed})...\", end=\" \")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=seed, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Normalize\n",
    "    scaler_split = StandardScaler()\n",
    "    X_train_split = scaler_split.fit_transform(X_train_split)\n",
    "    X_test_split = scaler_split.transform(X_test_split)\n",
    "    \n",
    "    # Compute contamination for this split\n",
    "    contam = (y_train_split == 1).sum() / len(y_train_split)\n",
    "    \n",
    "    # IForest\n",
    "    model_if = IForest(n_estimators=100, contamination=contam, random_state=seed)\n",
    "    model_if.fit(X_train_split)\n",
    "    y_pred = model_if.predict(X_test_split)\n",
    "    y_scores = model_if.decision_function(X_test_split)\n",
    "    results['IForest']['BA'].append(balanced_accuracy_score(y_test_split, y_pred))\n",
    "    results['IForest']['AUC'].append(roc_auc_score(y_test_split, y_scores))\n",
    "    \n",
    "    # LODA\n",
    "    model_loda = LODA(contamination=contam, n_bins=10, n_random_cuts=100)\n",
    "    model_loda.fit(X_train_split)\n",
    "    y_pred = model_loda.predict(X_test_split)\n",
    "    y_scores = model_loda.decision_function(X_test_split)\n",
    "    results['LODA']['BA'].append(balanced_accuracy_score(y_test_split, y_pred))\n",
    "    results['LODA']['AUC'].append(roc_auc_score(y_test_split, y_scores))\n",
    "    \n",
    "    # DIF\n",
    "    model_dif = DIF(contamination=contam, hidden_neurons=[64, 32], random_state=seed)\n",
    "    model_dif.fit(X_train_split)\n",
    "    y_pred = model_dif.predict(X_test_split)\n",
    "    y_scores = model_dif.decision_function(X_test_split)\n",
    "    results['DIF']['BA'].append(balanced_accuracy_score(y_test_split, y_pred))\n",
    "    results['DIF']['AUC'].append(roc_auc_score(y_test_split, y_scores))\n",
    "    \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Completed!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d597149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation for each model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS: Mean ± Std over 10 train-test splits\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name in ['IForest', 'LODA', 'DIF']:\n",
    "    ba_values = results[model_name]['BA']\n",
    "    auc_values = results[model_name]['AUC']\n",
    "    \n",
    "    ba_mean = np.mean(ba_values)\n",
    "    ba_std = np.std(ba_values)\n",
    "    auc_mean = np.mean(auc_values)\n",
    "    auc_std = np.std(auc_values)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"   Balanced Accuracy: {ba_mean:.4f} ± {ba_std:.4f}\")\n",
    "    print(f\"   ROC AUC:           {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "models = ['IForest', 'LODA', 'DIF']\n",
    "colors_bar = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "# Plot Balanced Accuracy\n",
    "ax = axes[0]\n",
    "ba_means = [np.mean(results[m]['BA']) for m in models]\n",
    "ba_stds = [np.std(results[m]['BA']) for m in models]\n",
    "\n",
    "bars = ax.bar(models, ba_means, yerr=ba_stds, capsize=8, \n",
    "              color=colors_bar, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Balanced Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Balanced Accuracy Comparison\\n(Mean ± Std over 10 splits)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean, std in zip(bars, ba_means, ba_stds):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + std + 0.02,\n",
    "            f'{mean:.4f}±{std:.4f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot ROC AUC\n",
    "ax = axes[1]\n",
    "auc_means = [np.mean(results[m]['AUC']) for m in models]\n",
    "auc_stds = [np.std(results[m]['AUC']) for m in models]\n",
    "\n",
    "bars = ax.bar(models, auc_means, yerr=auc_stds, capsize=8,\n",
    "              color=colors_bar, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('ROC AUC', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC AUC Comparison\\n(Mean ± Std over 10 splits)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean, std in zip(bars, auc_means, auc_stds):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + std + 0.02,\n",
    "            f'{mean:.4f}±{std:.4f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Shuttle Dataset: Model Performance Comparison', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
