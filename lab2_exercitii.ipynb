{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6f817c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(67676767)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leverage_scores(X):\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    \n",
    "    X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    \n",
    "    XtX_inv = np.linalg.pinv(X_with_intercept.T @ X_with_intercept)\n",
    "    H = X_with_intercept @ XtX_inv @ X_with_intercept.T\n",
    "    \n",
    "    leverage_scores = np.diag(H)\n",
    "    \n",
    "    return leverage_scores\n",
    "\n",
    "test_x = np.array([1, 2, 3, 4, 5])\n",
    "test_leverage = leverage_scores(test_x)\n",
    "print(f\"Test pentru leverage scores: {test_leverage}\")\n",
    "print(f\"leverage: {np.mean(test_leverage):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = ax + b + noise\n",
    "a = 2.0\n",
    "b = 1.0\n",
    "\n",
    "n_points = 25\n",
    "\n",
    "# Different noise levels to test\n",
    "noise_variances = [0.1, 0.5, 1.0, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ba5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_points(n_points, noise_var, point_type):\n",
    "    if point_type == \"regular\":\n",
    "        # Regular points: x in small range, low noise\n",
    "        x = np.random.uniform(0, 2, n_points)\n",
    "        noise = np.random.normal(0, np.sqrt(noise_var), n_points)\n",
    "        \n",
    "    elif point_type == \"high_x_var\":\n",
    "        # High X variance: x spread out more\n",
    "        x = np.random.uniform(-2, 4, n_points)\n",
    "        noise = np.random.normal(0, np.sqrt(noise_var), n_points)\n",
    "        \n",
    "    elif point_type == \"high_y_var\":\n",
    "        # High Y variance: more noise in y direction\n",
    "        x = np.random.uniform(0, 2, n_points)\n",
    "        noise = np.random.normal(0, np.sqrt(noise_var * 5), n_points)  # 5x more noise\n",
    "        \n",
    "    elif point_type == \"high_both\":\n",
    "        # High variance in both x and y\n",
    "        x = np.random.uniform(-2, 4, n_points)\n",
    "        noise = np.random.normal(0, np.sqrt(noise_var * 5), n_points)\n",
    "    \n",
    "    y = a * x + b + noise\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "test_noise = 0.5\n",
    "x_reg, y_reg = generate_data_points(n_points, test_noise, \"regular\")\n",
    "x_hx, y_hx = generate_data_points(n_points, test_noise, \"high_x_var\")\n",
    "x_hy, y_hy = generate_data_points(n_points, test_noise, \"high_y_var\")\n",
    "x_hb, y_hb = generate_data_points(n_points, test_noise, \"high_both\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_reg, y_reg, label=\"Regular Points\", color='blue')\n",
    "plt.scatter(x_hx, y_hx, label=\"High X Variance\", color='orange')\n",
    "plt.scatter(x_hy, y_hy, label=\"High Y Variance\", color='green')\n",
    "plt.scatter(x_hb, y_hb, label=\"High Both Variance\", color='red')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Data Points with Different Variance Characteristics\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ecdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "point_types = [\"regular\", \"high_x_var\", \"high_y_var\", \"high_both\"]\n",
    "\n",
    "for noise_var in noise_variances:\n",
    "    results[noise_var] = {}\n",
    "    print(f\"\\nNoise variance: {noise_var}\")\n",
    "    \n",
    "    for point_type in point_types:\n",
    "        x, y = generate_data_points(n_points, noise_var, point_type)\n",
    "        \n",
    "        leverage = leverage_scores(x)\n",
    "        \n",
    "        results[noise_var][point_type] = {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'leverage': leverage,\n",
    "            'max_leverage': np.max(leverage),\n",
    "            'avg_leverage': np.mean(leverage)\n",
    "        }\n",
    "        \n",
    "        print(f\"  {point_type:12}: avg leverage = {np.mean(leverage):.3f}, max = {np.max(leverage):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'regular': 'blue', 'high_x_var': 'red', 'high_y_var': 'green', 'high_both': 'orange'}\n",
    "labels = {'regular': 'Regular', 'high_x_var': 'High X var', 'high_y_var': 'High Y var', 'high_both': 'High Both'}\n",
    "\n",
    "for i, noise_var in enumerate(noise_variances):\n",
    "    # Create a new figure for each noise variance\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    for point_type in point_types:\n",
    "        data = results[noise_var][point_type]\n",
    "        x, y, leverage = data['x'], data['y'], data['leverage']\n",
    "        \n",
    "        scatter = ax.scatter(x, y, \n",
    "                           c=colors[point_type], \n",
    "                           s=leverage * 300,\n",
    "                           alpha=0.6, \n",
    "                           label=labels[point_type])\n",
    "    \n",
    "    x_line = np.linspace(-3, 5, 100)\n",
    "    y_line = a * x_line + b\n",
    "    ax.plot(x_line, y_line, 'k--', linewidth=2, alpha=0.8, label='True model')\n",
    "    \n",
    "    # highlight highest leverage points\n",
    "    all_x = np.concatenate([results[noise_var][pt]['x'] for pt in point_types])\n",
    "    all_leverage = np.concatenate([results[noise_var][pt]['leverage'] for pt in point_types])\n",
    "    top_5_idx = np.argsort(all_leverage)[-5:]\n",
    "    \n",
    "    all_y = np.concatenate([results[noise_var][pt]['y'] for pt in point_types])\n",
    "    ax.scatter(all_x[top_5_idx], all_y[top_5_idx], \n",
    "              s=100, facecolors='none', edgecolors='black', linewidth=2,\n",
    "              label='Highest leverage')\n",
    "    \n",
    "    ax.set_title(f'Leverage Scores Analysis - Noise variance = {noise_var}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e06491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Model parameters: y = a*x1 + b*x2 + c + noise\n",
    "a_2d = 1.5  # coefficient for x1\n",
    "b_2d = 2.0  # coefficient for x2  \n",
    "c_2d = 0.5  # intercept\n",
    "\n",
    "def generate_2d_data(n_points, noise_var, point_type):\n",
    "    if point_type == \"regular\":\n",
    "        x1 = np.random.uniform(0, 2, n_points)\n",
    "        x2 = np.random.uniform(0, 2, n_points)\n",
    "        noise = np.random.normal(0, np.sqrt(noise_var), n_points)\n",
    "        \n",
    "    elif point_type == \"high_x_var\":\n",
    "        x1 = np.random.uniform(-2, 4, n_points)\n",
    "        x2 = np.random.uniform(-2, 4, n_points)\n",
    "        noise = np.random.normal(0, np.sqrt(noise_var), n_points)\n",
    "        \n",
    "    elif point_type == \"high_y_var\":\n",
    "        x1 = np.random.uniform(0, 2, n_points)\n",
    "        x2 = np.random.uniform(0, 2, n_points)\n",
    "        noise = np.random.normal(0, np.sqrt(noise_var * 5), n_points)\n",
    "        \n",
    "    elif point_type == \"high_both\":\n",
    "        x1 = np.random.uniform(-2, 4, n_points)\n",
    "        x2 = np.random.uniform(-2, 4, n_points)\n",
    "        noise = np.random.normal(0, np.sqrt(noise_var * 5), n_points)\n",
    "    \n",
    "    y = a_2d * x1 + b_2d * x2 + c_2d + noise\n",
    "    \n",
    "    return np.column_stack([x1, x2]), y\n",
    "\n",
    "def calculate_2d_leverage(X):\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    \n",
    "    X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    \n",
    "    XtX_inv = np.linalg.pinv(X_with_intercept.T @ X_with_intercept)\n",
    "    H = X_with_intercept @ XtX_inv @ X_with_intercept.T\n",
    "    \n",
    "    return np.diag(H)\n",
    "\n",
    "print(f\"2D Model: y = {a_2d}*x1 + {b_2d}*x2 + {c_2d} + Îµ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac026a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D data and calculate leverage scores\n",
    "results_2d = {}\n",
    "\n",
    "for noise_var in noise_variances:\n",
    "    results_2d[noise_var] = {}\n",
    "    print(f\"\\nNoise variance: {noise_var}\")\n",
    "    \n",
    "    for point_type in point_types:\n",
    "        X, y = generate_2d_data(n_points, noise_var, point_type)\n",
    "        leverage = calculate_2d_leverage(X)\n",
    "        \n",
    "        results_2d[noise_var][point_type] = {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'leverage': leverage,\n",
    "            'max_leverage': np.max(leverage),\n",
    "            'avg_leverage': np.mean(leverage)\n",
    "        }\n",
    "        \n",
    "        print(f\"  {point_type:12}: avg leverage = {np.mean(leverage):.3f}, max = {np.max(leverage):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcdc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D results\n",
    "for i, noise_var in enumerate(noise_variances):\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    for point_type in point_types:\n",
    "        data = results_2d[noise_var][point_type]\n",
    "        X, y, leverage = data['X'], data['y'], data['leverage']\n",
    "        \n",
    "        # Plot 3D scatter: x1, x2, y with size based on leverage\n",
    "        ax.scatter(X[:, 0], X[:, 1], y, \n",
    "                  c=colors[point_type], \n",
    "                  s=leverage * 400,  # Scale up for visibility\n",
    "                  alpha=0.6, \n",
    "                  label=labels[point_type])\n",
    "    \n",
    "    # Find and highlight highest leverage points\n",
    "    all_X = np.vstack([results_2d[noise_var][pt]['X'] for pt in point_types])\n",
    "    all_y = np.concatenate([results_2d[noise_var][pt]['y'] for pt in point_types])\n",
    "    all_leverage = np.concatenate([results_2d[noise_var][pt]['leverage'] for pt in point_types])\n",
    "    \n",
    "    top_5_idx = np.argsort(all_leverage)[-5:]\n",
    "    ax.scatter(all_X[top_5_idx, 0], all_X[top_5_idx, 1], all_y[top_5_idx], \n",
    "              s=150, facecolors='none', edgecolors='black', linewidth=3,\n",
    "              label='Highest leverage')\n",
    "    \n",
    "    ax.set_title(f'2D Leverage Scores Analysis - Noise variance = {noise_var}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('X1')\n",
    "    ax.set_ylabel('X2')\n",
    "    ax.set_zlabel('Y')\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c30f3",
   "metadata": {},
   "source": [
    "## Ex. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.utils.data import generate_data_clusters\n",
    "from pyod.models.knn import KNN\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clustered data with contamination\n",
    "# 400 train, 200 test, 2 clusters, 2 features, 10% contamination\n",
    "X_train, X_test, y_train, y_test = generate_data_clusters(\n",
    "    n_train=400,\n",
    "    n_test=200,\n",
    "    n_clusters=2,\n",
    "    n_features=2,\n",
    "    contamination=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff449bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different n_neighbors values\n",
    "n_neighbors_list = [3, 5, 10, 20, 30, 50]\n",
    "\n",
    "knn_results = {}\n",
    "\n",
    "for n_neighbors in n_neighbors_list:\n",
    "    # Create and train KNN model\n",
    "    knn = KNN(n_neighbors=n_neighbors, contamination=0.1)\n",
    "    knn.fit(X_train)\n",
    "    \n",
    "    # Get predictions (0 = inlier, 1 = outlier)\n",
    "    y_train_pred = knn.labels_ \n",
    "    y_test_pred = knn.predict(X_test)\n",
    "    \n",
    "    train_acc = balanced_accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    knn_results[n_neighbors] = {\n",
    "        'model': knn,\n",
    "        'y_train_pred': y_train_pred,\n",
    "        'y_test_pred': y_test_pred,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"n_neighbors={n_neighbors:2d} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45883736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results for each n_neighbors value\n",
    "for n_neighbors in n_neighbors_list:\n",
    "    result = knn_results[n_neighbors]\n",
    "    y_train_pred = result['y_train_pred']\n",
    "    y_test_pred = result['y_test_pred']\n",
    "    train_acc = result['train_acc']\n",
    "    test_acc = result['test_acc']\n",
    "    \n",
    "    # Create 2x2 subplot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # Subplot 1: Ground truth - Training data\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], \n",
    "               c='blue', label='Inliers', alpha=0.6, s=30)\n",
    "    ax.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], \n",
    "               c='red', label='Outliers', alpha=0.8, s=50, marker='x')\n",
    "    ax.set_title('Ground Truth - Training Data', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Predicted - Training data\n",
    "    ax = axes[0, 1]\n",
    "    ax.scatter(X_train[y_train_pred==0, 0], X_train[y_train_pred==0, 1], \n",
    "               c='blue', label='Inliers', alpha=0.6, s=30)\n",
    "    ax.scatter(X_train[y_train_pred==1, 0], X_train[y_train_pred==1, 1], \n",
    "               c='red', label='Outliers', alpha=0.8, s=50, marker='x')\n",
    "    ax.set_title(f'Predicted - Training Data (Acc: {train_acc:.4f})', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Ground truth - Test data\n",
    "    ax = axes[1, 0]\n",
    "    ax.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], \n",
    "               c='blue', label='Inliers', alpha=0.6, s=30)\n",
    "    ax.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], \n",
    "               c='red', label='Outliers', alpha=0.8, s=50, marker='x')\n",
    "    ax.set_title('Ground Truth - Test Data', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: Predicted - Test data\n",
    "    ax = axes[1, 1]\n",
    "    ax.scatter(X_test[y_test_pred==0, 0], X_test[y_test_pred==0, 1], \n",
    "               c='blue', label='Inliers', alpha=0.6, s=30)\n",
    "    ax.scatter(X_test[y_test_pred==1, 0], X_test[y_test_pred==1, 1], \n",
    "               c='red', label='Outliers', alpha=0.8, s=50, marker='x')\n",
    "    ax.set_title(f'Predicted - Test Data (Acc: {test_acc:.4f})', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'KNN Anomaly Detection (n_neighbors = {n_neighbors})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    print(f\"Plot displayed for n_neighbors = {n_neighbors}\")\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b21f6",
   "metadata": {},
   "source": [
    "## Ex. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af40dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.lof import LOF\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73497ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2 clusters with different densities\n",
    "# Cluster 1: center (-10, -10), std = 2\n",
    "# Cluster 2: center (10, 10), std = 6\n",
    "\n",
    "n_samples_per_cluster = 200\n",
    "centers = [(-10, -10), (10, 10)]\n",
    "cluster_std = [2, 6]\n",
    "\n",
    "X_ex3, y_clusters = make_blobs(\n",
    "    n_samples=[n_samples_per_cluster, n_samples_per_cluster],\n",
    "    centers=centers,\n",
    "    cluster_std=cluster_std,\n",
    "    n_features=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Cluster 1: center {centers[0]}, std = {cluster_std[0]}\")\n",
    "print(f\"Cluster 2: center {centers[1]}, std = {cluster_std[1]}\")\n",
    "\n",
    "# Add some outliers manually \n",
    "# (7% contamination)\n",
    "np.random.seed(42)\n",
    "n_outliers = int(0.07 * len(X_ex3))\n",
    "outlier_indices = np.random.choice(len(X_ex3), n_outliers, replace=False)\n",
    "\n",
    "# Create ground truth labels \n",
    "# (0 = inlier, 1 = outlier)\n",
    "y_true_ex3 = np.zeros(len(X_ex3))\n",
    "y_true_ex3[outlier_indices] = 1\n",
    "\n",
    "# Perturb outliers to make them more extreme\n",
    "X_ex3[outlier_indices] += np.random.normal(0, 5, (n_outliers, 2))\n",
    "\n",
    "print(f\"Number of outliers: {n_outliers} ({n_outliers/len(X_ex3):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the generated data with ground truth\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_ex3[y_true_ex3==0, 0], X_ex3[y_true_ex3==0, 1], \n",
    "            c='blue', label='Inliers', alpha=0.6, s=30)\n",
    "plt.scatter(X_ex3[y_true_ex3==1, 0], X_ex3[y_true_ex3==1, 1], \n",
    "            c='red', label='Outliers', alpha=0.8, s=50, marker='x')\n",
    "plt.title('Generated Data with Different Cluster Densities', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Left cluster is denser (std=2), right cluster is sparser (std=6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different n_neighbors values for both KNN and LOF\n",
    "n_neighbors_list_ex3 = [5, 10, 20, 30, 50]\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "for n_neighbors in n_neighbors_list_ex3:\n",
    "    # Train KNN model\n",
    "    knn_model = KNN(n_neighbors=n_neighbors, contamination=0.07)\n",
    "    knn_model.fit(X_ex3)\n",
    "    y_pred_knn = knn_model.labels_\n",
    "    \n",
    "    # Train LOF model\n",
    "    lof_model = LOF(n_neighbors=n_neighbors, contamination=0.07)\n",
    "    lof_model.fit(X_ex3)\n",
    "    y_pred_lof = lof_model.labels_\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    knn_acc = balanced_accuracy_score(y_true_ex3, y_pred_knn)\n",
    "    lof_acc = balanced_accuracy_score(y_true_ex3, y_pred_lof)\n",
    "    \n",
    "    comparison_results[n_neighbors] = {\n",
    "        'knn_model': knn_model,\n",
    "        'lof_model': lof_model,\n",
    "        'y_pred_knn': y_pred_knn,\n",
    "        'y_pred_lof': y_pred_lof,\n",
    "        'knn_acc': knn_acc,\n",
    "        'lof_acc': lof_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"n_neighbors={n_neighbors:2d} | KNN Acc: {knn_acc:.4f} | LOF Acc: {lof_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KNN vs LOF for each n_neighbors\n",
    "for n_neighbors in n_neighbors_list_ex3:\n",
    "    result = comparison_results[n_neighbors]\n",
    "    y_pred_knn = result['y_pred_knn']\n",
    "    y_pred_lof = result['y_pred_lof']\n",
    "    knn_acc = result['knn_acc']\n",
    "    lof_acc = result['lof_acc']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot KNN predictions\n",
    "    ax1.scatter(X_ex3[y_pred_knn==0, 0], X_ex3[y_pred_knn==0, 1], \n",
    "                c='blue', label='Inliers', alpha=0.6, s=30)\n",
    "    ax1.scatter(X_ex3[y_pred_knn==1, 0], X_ex3[y_pred_knn==1, 1], \n",
    "                c='red', label='Outliers', alpha=0.8, s=50, marker='x')\n",
    "    ax1.set_title(f'KNN (n_neighbors={n_neighbors}, Acc: {knn_acc:.4f})', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Feature 1')\n",
    "    ax1.set_ylabel('Feature 2')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot LOF predictions\n",
    "    ax2.scatter(X_ex3[y_pred_lof==0, 0], X_ex3[y_pred_lof==0, 1], \n",
    "                c='blue', label='Inliers', alpha=0.6, s=30)\n",
    "    ax2.scatter(X_ex3[y_pred_lof==1, 0], X_ex3[y_pred_lof==1, 1], \n",
    "                c='red', label='Outliers', alpha=0.8, s=50, marker='x')\n",
    "    ax2.set_title(f'LOF (n_neighbors={n_neighbors}, Acc: {lof_acc:.4f})', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Feature 1')\n",
    "    ax2.set_ylabel('Feature 2')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'KNN vs LOF Comparison - Different Cluster Densities', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    print(f\"Plot displayed for n_neighbors = {n_neighbors}\")\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare accuracy across different n_neighbors\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "knn_accs = [comparison_results[n]['knn_acc'] for n in n_neighbors_list_ex3]\n",
    "lof_accs = [comparison_results[n]['lof_acc'] for n in n_neighbors_list_ex3]\n",
    "\n",
    "ax.plot(n_neighbors_list_ex3, knn_accs, 'o-', label='KNN', \n",
    "        linewidth=2, markersize=10, color='blue')\n",
    "ax.plot(n_neighbors_list_ex3, lof_accs, 's-', label='LOF', \n",
    "        linewidth=2, markersize=10, color='green')\n",
    "ax.set_xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "ax.set_ylabel('Balanced Accuracy', fontsize=12)\n",
    "ax.set_title('KNN vs LOF Performance on Different Density Clusters', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(n_neighbors_list_ex3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b2fde",
   "metadata": {},
   "source": [
    "## Ex. 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.models.combination import average, maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702afb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('cardio.mat')\n",
    "\n",
    "X = data['X']\n",
    "y = data['y'].ravel()  # Flatten to 1D array\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of outliers: {np.sum(y)}\")\n",
    "print(f\"Contamination rate: {np.sum(y) / len(y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f5a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train_cardio, X_test_cardio, y_train_cardio, y_test_cardio = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_cardio.shape}\")\n",
    "print(f\"Test set: {X_test_cardio.shape}\")\n",
    "print(f\"Train outliers: {np.sum(y_train_cardio)} / {len(y_train_cardio)}\")\n",
    "print(f\"Test outliers: {np.sum(y_test_cardio)} / {len(y_test_cardio)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data (zero mean, unit variance)\n",
    "X_train_norm, X_test_norm = standardizer(X_train_cardio, X_test_cardio)\n",
    "\n",
    "print(f\"Train mean: {np.mean(X_train_norm, axis=0)[:3]}\")\n",
    "print(f\"Train std: {np.std(X_train_norm, axis=0)[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ae05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual contamination rate for the model\n",
    "contamination_rate = np.sum(y_train_cardio) / len(y_train_cardio)\n",
    "print(f\"Contamination rate: {contamination_rate:.4f}\")\n",
    "\n",
    "# Create 10 KNN models with different n_neighbors (30 to 120)\n",
    "n_neighbors_range = range(30, 121, 10)  # 30, 40, 50, ..., 120\n",
    "n_models = len(n_neighbors_range)\n",
    "\n",
    "print(f\"\\nTraining {n_models} KNN models with n_neighbors from 30 to 120...\")\n",
    "\n",
    "knn_models = []\n",
    "train_scores_knn = []\n",
    "test_scores_knn = []\n",
    "\n",
    "for n_neighbors in n_neighbors_range:\n",
    "    # Create and train KNN model\n",
    "    knn_model = KNN(n_neighbors=n_neighbors, contamination=contamination_rate)\n",
    "    knn_model.fit(X_train_norm)\n",
    "    \n",
    "    # Get anomaly scores (not labels)\n",
    "    train_scores_knn.append(knn_model.decision_scores_)\n",
    "    test_scores_knn.append(knn_model.decision_function(X_test_norm))\n",
    "    \n",
    "    knn_models.append(knn_model)\n",
    "    \n",
    "    print(f\"  KNN with n_neighbors={n_neighbors:3d} trained\")\n",
    "\n",
    "# Convert to numpy arrays (shape: n_models x n_samples)\n",
    "train_scores_knn = np.array(train_scores_knn).T  # Transpose to (n_samples x n_models)\n",
    "test_scores_knn = np.array(test_scores_knn).T\n",
    "\n",
    "print(f\"\\nTrain scores shape: {train_scores_knn.shape}\")\n",
    "print(f\"Test scores shape: {test_scores_knn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f733f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 LOF models with different n_neighbors (30 to 120)\n",
    "print(f\"Training {n_models} LOF models with n_neighbors from 30 to 120...\")\n",
    "\n",
    "lof_models = []\n",
    "train_scores_lof = []\n",
    "test_scores_lof = []\n",
    "\n",
    "for n_neighbors in n_neighbors_range:\n",
    "    # Create and train LOF model\n",
    "    lof_model = LOF(n_neighbors=n_neighbors, contamination=contamination_rate)\n",
    "    lof_model.fit(X_train_norm)\n",
    "    \n",
    "    # Get anomaly scores (not labels)\n",
    "    train_scores_lof.append(lof_model.decision_scores_)\n",
    "    test_scores_lof.append(lof_model.decision_function(X_test_norm))\n",
    "    \n",
    "    lof_models.append(lof_model)\n",
    "    \n",
    "    print(f\"  LOF with n_neighbors={n_neighbors:3d} trained\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_scores_lof = np.array(train_scores_lof).T\n",
    "test_scores_lof = np.array(test_scores_lof).T\n",
    "\n",
    "print(f\"\\nTrain scores shape: {train_scores_lof.shape}\")\n",
    "print(f\"Test scores shape: {test_scores_lof.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b76c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_knn_norm, test_scores_knn_norm = standardizer(train_scores_knn, test_scores_knn)\n",
    "train_scores_lof_norm, test_scores_lof_norm = standardizer(train_scores_lof, test_scores_lof)\n",
    "print(\"gata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5290a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: AVERAGE combination for KNN ensemble\n",
    "train_avg_knn = average(train_scores_knn_norm)\n",
    "test_avg_knn = average(test_scores_knn_norm)\n",
    "\n",
    "# Strategy 2: MAXIMIZATION combination for KNN ensemble\n",
    "train_max_knn = maximization(train_scores_knn_norm)\n",
    "test_max_knn = maximization(test_scores_knn_norm)\n",
    "\n",
    "print(f\"Average strategy - Train shape: {train_avg_knn.shape}, Test shape: {test_avg_knn.shape}\")\n",
    "print(f\"Max strategy - Train shape: {train_max_knn.shape}, Test shape: {test_max_knn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bd281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: AVERAGE combination for LOF ensemble\n",
    "train_avg_lof = average(train_scores_lof_norm)\n",
    "test_avg_lof = average(test_scores_lof_norm)\n",
    "\n",
    "# Strategy 2: MAXIMIZATION combination for LOF ensemble\n",
    "train_max_lof = maximization(train_scores_lof_norm)\n",
    "test_max_lof = maximization(test_scores_lof_norm)\n",
    "\n",
    "print(f\"Average strategy - Train shape: {train_avg_lof.shape}, Test shape: {test_avg_lof.shape}\")\n",
    "print(f\"Max strategy - Train shape: {train_max_lof.shape}, Test shape: {test_max_lof.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38bf9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold using contamination rate and quantile\n",
    "# Threshold = score at (1 - contamination) quantile\n",
    "\n",
    "# For KNN - Average strategy\n",
    "threshold_train_avg_knn = np.quantile(train_avg_knn, 1 - contamination_rate)\n",
    "threshold_test_avg_knn = np.quantile(test_avg_knn, 1 - contamination_rate)\n",
    "\n",
    "# For KNN - Max strategy\n",
    "threshold_train_max_knn = np.quantile(train_max_knn, 1 - contamination_rate)\n",
    "threshold_test_max_knn = np.quantile(test_max_knn, 1 - contamination_rate)\n",
    "\n",
    "# For LOF - Average strategy\n",
    "threshold_train_avg_lof = np.quantile(train_avg_lof, 1 - contamination_rate)\n",
    "threshold_test_avg_lof = np.quantile(test_avg_lof, 1 - contamination_rate)\n",
    "\n",
    "# For LOF - Max strategy\n",
    "threshold_train_max_lof = np.quantile(train_max_lof, 1 - contamination_rate)\n",
    "threshold_test_max_lof = np.quantile(test_max_lof, 1 - contamination_rate)\n",
    "\n",
    "print(\"Thresholds calculated using contamination rate!\")\n",
    "print(f\"Contamination rate: {contamination_rate:.4f}\")\n",
    "\n",
    "print()\n",
    "print(f\"KNN - Average: Train threshold = {threshold_train_avg_knn:.4f}, Test threshold = {threshold_test_avg_knn:.4f}\")\n",
    "print(f\"KNN - Max: Train threshold = {threshold_train_max_knn:.4f}, Test threshold = {threshold_test_max_knn:.4f}\")\n",
    "print(f\"LOF - Average: Train threshold = {threshold_train_avg_lof:.4f}, Test threshold = {threshold_test_avg_lof:.4f}\")\n",
    "print(f\"LOF - Max: Train threshold = {threshold_train_max_lof:.4f}, Test threshold = {threshold_test_max_lof:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afee648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions based on thresholds\n",
    "# If score > threshold, then outlier (1), else inlier (0)\n",
    "\n",
    "# KNN - Average\n",
    "y_train_pred_avg_knn = (train_avg_knn > threshold_train_avg_knn).astype(int)\n",
    "y_test_pred_avg_knn = (test_avg_knn > threshold_test_avg_knn).astype(int)\n",
    "\n",
    "# KNN - Max\n",
    "y_train_pred_max_knn = (train_max_knn > threshold_train_max_knn).astype(int)\n",
    "y_test_pred_max_knn = (test_max_knn > threshold_test_max_knn).astype(int)\n",
    "\n",
    "# LOF - Average\n",
    "y_train_pred_avg_lof = (train_avg_lof > threshold_train_avg_lof).astype(int)\n",
    "y_test_pred_avg_lof = (test_avg_lof > threshold_test_avg_lof).astype(int)\n",
    "\n",
    "# LOF - Max\n",
    "y_train_pred_max_lof = (train_max_lof > threshold_train_max_lof).astype(int)\n",
    "y_test_pred_max_lof = (test_max_lof > threshold_test_max_lof).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate balanced accuracy for all strategies\n",
    "\n",
    "# KNN - Average\n",
    "ba_train_avg_knn = balanced_accuracy_score(y_train_cardio, y_train_pred_avg_knn)\n",
    "ba_test_avg_knn = balanced_accuracy_score(y_test_cardio, y_test_pred_avg_knn)\n",
    "\n",
    "# KNN - Max\n",
    "ba_train_max_knn = balanced_accuracy_score(y_train_cardio, y_train_pred_max_knn)\n",
    "ba_test_max_knn = balanced_accuracy_score(y_test_cardio, y_test_pred_max_knn)\n",
    "\n",
    "# LOF - Average\n",
    "ba_train_avg_lof = balanced_accuracy_score(y_train_cardio, y_train_pred_avg_lof)\n",
    "ba_test_avg_lof = balanced_accuracy_score(y_test_cardio, y_test_pred_avg_lof)\n",
    "\n",
    "# LOF - Max\n",
    "ba_train_max_lof = balanced_accuracy_score(y_train_cardio, y_train_pred_max_lof)\n",
    "ba_test_max_lof = balanced_accuracy_score(y_test_cardio, y_test_pred_max_lof)\n",
    "\n",
    "print(f\"{'Strategy':<20} {'Train BA':<15} {'Test BA':<15}\")\n",
    "print(f\"{'KNN - Average':<20} {ba_train_avg_knn:<15.4f} {ba_test_avg_knn:<15.4f}\")\n",
    "print(f\"{'KNN - Maximization':<20} {ba_train_max_knn:<15.4f} {ba_test_max_knn:<15.4f}\")\n",
    "print(f\"{'LOF - Average':<20} {ba_train_avg_lof:<15.4f} {ba_test_avg_lof:<15.4f}\")\n",
    "print(f\"{'LOF - Maximization':<20} {ba_train_max_lof:<15.4f} {ba_test_max_lof:<15.4f}\")\n",
    "\n",
    "# Find best strategy\n",
    "strategies = {\n",
    "    'KNN - Average': ba_test_avg_knn,\n",
    "    'KNN - Maximization': ba_test_max_knn,\n",
    "    'LOF - Average': ba_test_avg_lof,\n",
    "    'LOF - Maximization': ba_test_max_lof\n",
    "}\n",
    "\n",
    "best_strategy = max(strategies, key=strategies.get)\n",
    "best_score = strategies[best_strategy]\n",
    "\n",
    "print()\n",
    "print(f\"Best strategy: {best_strategy} with Test BA = {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies_list = ['KNN\\nAverage', 'KNN\\nMax', 'LOF\\nAverage', 'LOF\\nMax']\n",
    "train_scores_list = [ba_train_avg_knn, ba_train_max_knn, ba_train_avg_lof, ba_train_max_lof]\n",
    "test_scores_list = [ba_test_avg_knn, ba_test_max_knn, ba_test_avg_lof, ba_test_max_lof]\n",
    "\n",
    "plt.plot(strategies_list, train_scores_list, 'o-', label='Train', \n",
    "         linewidth=2, markersize=10, color='blue')\n",
    "plt.plot(strategies_list, test_scores_list, 's-', label='Test', \n",
    "         linewidth=2, markersize=10, color='red')\n",
    "plt.xlabel('Strategy', fontsize=12)\n",
    "plt.ylabel('Balanced Accuracy', fontsize=12)\n",
    "plt.title('Performance Trends', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0.5, 1.0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare individual models vs ensemble\n",
    "# Calculate BA for individual models (middle one: n_neighbors=70)\n",
    "middle_idx = len(knn_models) // 2\n",
    "middle_n = list(n_neighbors_range)[middle_idx]\n",
    "\n",
    "# Get predictions from individual models\n",
    "y_train_pred_single_knn = knn_models[middle_idx].predict(X_train_norm)\n",
    "y_test_pred_single_knn = knn_models[middle_idx].predict(X_test_norm)\n",
    "\n",
    "y_train_pred_single_lof = lof_models[middle_idx].predict(X_train_norm)\n",
    "y_test_pred_single_lof = lof_models[middle_idx].predict(X_test_norm)\n",
    "\n",
    "# Calculate BA for single models\n",
    "ba_train_single_knn = balanced_accuracy_score(y_train_cardio, y_train_pred_single_knn)\n",
    "ba_test_single_knn = balanced_accuracy_score(y_test_cardio, y_test_pred_single_knn)\n",
    "\n",
    "ba_train_single_lof = balanced_accuracy_score(y_train_cardio, y_train_pred_single_lof)\n",
    "ba_test_single_lof = balanced_accuracy_score(y_test_cardio, y_test_pred_single_lof)\n",
    "\n",
    "print(f\"{'Model':<25} {'Train BA':<15} {'Test BA':<15}\")\n",
    "print(f\"{'KNN - Single':<25} {ba_train_single_knn:<15.4f} {ba_test_single_knn:<15.4f}\")\n",
    "print(f\"{'KNN - Ensemble (Avg)':<25} {ba_train_avg_knn:<15.4f} {ba_test_avg_knn:<15.4f}\")\n",
    "print(f\"{'KNN - Ensemble (Max)':<25} {ba_train_max_knn:<15.4f} {ba_test_max_knn:<15.4f}\")\n",
    "print(f\"{'LOF - Single':<25} {ba_train_single_lof:<15.4f} {ba_test_single_lof:<15.4f}\")\n",
    "print(f\"{'LOF - Ensemble (Avg)':<25} {ba_train_avg_lof:<15.4f} {ba_test_avg_lof:<15.4f}\")\n",
    "print(f\"{'LOF - Ensemble (Max)':<25} {ba_train_max_lof:<15.4f} {ba_test_max_lof:<15.4f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "knn_improvement_avg = ba_test_avg_knn - ba_test_single_knn\n",
    "knn_improvement_max = ba_test_max_knn - ba_test_single_knn\n",
    "lof_improvement_avg = ba_test_avg_lof - ba_test_single_lof\n",
    "lof_improvement_max = ba_test_max_lof - ba_test_single_lof\n",
    "\n",
    "print(f\"\\nImprovement over single model (Test BA):\")\n",
    "print(f\"  KNN - Average: {knn_improvement_avg:+.4f}\")\n",
    "print(f\"  KNN - Max: {knn_improvement_max:+.4f}\")\n",
    "print(f\"  LOF - Average: {lof_improvement_avg:+.4f}\")\n",
    "print(f\"  LOF - Max: {lof_improvement_max:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
